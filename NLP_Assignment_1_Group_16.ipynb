{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment 1 (Group 16)\n",
        "\n",
        "## Bhargava Siva Naga Sai Potluri (bxp230045)\n",
        "\n",
        "## Nikhil Sesha Sai Kondapalli (nxk240025)\n",
        "\n",
        "## Kavimayil Periyakoravampalayam Komarasamy (kxp230053)\n",
        "\n",
        "## Sakshi Tokekar (sxt230143)"
      ],
      "metadata": {
        "id": "XsNlJCVIZnJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "KMjaDGnKIVlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e490cab-aa41-488a-989c-871b9eb44ee7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXkS11I8P9Pg",
        "outputId": "262324b1-b5d5-49b8-82c1-bc55ae75234b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Created Assignment 1 class and implemented different methods used in the Assignment"
      ],
      "metadata": {
        "id": "X7opwpYz04hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "class Assignment1:\n",
        "    def __init__(self):\n",
        "        self.unk_token = '<UNK>'\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def read_data(self, path):\n",
        "        # read data from input file\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "        return [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    def preprocessing(self, data, remove_stopwords=False):\n",
        "        # pre-process individual sentence\n",
        "        processed_sentences = []\n",
        "        for line in data:\n",
        "            processed_line = self.preprocess_single_sentence(line, remove_stopwords)\n",
        "            if processed_line.strip():\n",
        "                processed_sentences.append(processed_line)\n",
        "        return ' '.join(processed_sentences)\n",
        "\n",
        "    def preprocess_single_sentence(self, sentence, remove_stopwords=False):\n",
        "        # Lowercasing\n",
        "        sentence = sentence.lower()\n",
        "        # Contractions expansion\n",
        "        sentence = contractions.fix(sentence)\n",
        "        # Normalize numbers -> <num>\n",
        "        sentence = re.sub(r'\\d+', '<num>', sentence)\n",
        "        # Add sentence boundaries <s> </s> for each line/review\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            sentence = '<s> ' + sentence + ' </s>'\n",
        "        return sentence\n",
        "\n",
        "    def tokenize(self, data):\n",
        "        return data.split()\n",
        "\n",
        "    def unigram_model(self, tokens):\n",
        "        unigram_counts = {}\n",
        "\n",
        "        # Count occurrences of each token\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        # Calculate total number of tokens\n",
        "        total_tokens = len(tokens)\n",
        "\n",
        "        # Compute probabilities\n",
        "        unigram_probabilities = {}\n",
        "        for unigram, count in unigram_counts.items():\n",
        "            unigram_probabilities[unigram] = count / total_tokens\n",
        "\n",
        "        return unigram_probabilities, unigram_counts\n",
        "\n",
        "    def bigram_model(self, tokens):\n",
        "        bigram_counts = {}\n",
        "        unigram_counts = {}\n",
        "\n",
        "        # Count unigrams\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        # Count bigrams\n",
        "        for i in range(len(tokens) - 1):\n",
        "            bigram = (tokens[i], tokens[i+1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "        bigram_probabilities = {}\n",
        "\n",
        "        # Compute conditional probabilities P(w2|w1) = Count(w1,w2) / Count(w1)\n",
        "        for bigram, count in bigram_counts.items():\n",
        "            first_token = bigram[0]\n",
        "            if first_token in unigram_counts and unigram_counts[first_token] > 0:\n",
        "                bigram_probabilities[bigram] = count / unigram_counts[first_token]\n",
        "            else:\n",
        "                bigram_probabilities[bigram] = 0.0\n",
        "\n",
        "        return bigram_probabilities, bigram_counts\n",
        "\n",
        "    def handle_unknown_words(self, tokens, unigram_counts, min_freq=1):\n",
        "        \"\"\"\n",
        "        Replaces words with a frequency less than or equal to min_freq\n",
        "        with the <UNK> token.\n",
        "        \"\"\"\n",
        "        # Create a set of rare words for efficient lookup\n",
        "        rare_words = set()\n",
        "\n",
        "        # Loop over each word-count pair in the unigram_counts dictionary\n",
        "        for word, count in unigram_counts.items():\n",
        "            if count <= min_freq:\n",
        "                rare_words.add(word)  # add the rare word to the set\n",
        "\n",
        "        if rare_words:\n",
        "            print(f\"\\nFound {len(rare_words)} unique words with frequency <= {min_freq}. Replacing them with '<UNK>'.\")\n",
        "\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if token in rare_words:        # if token is rare\n",
        "                processed_tokens.append('<UNK>')  # replace with <UNK>\n",
        "            else:\n",
        "                processed_tokens.append(token)    # otherwise keep as is\n",
        "\n",
        "        return processed_tokens\n",
        "\n",
        "    def export_to_csv(self, unigram_probs, unigram_counts, bigram_probs, bigram_counts, output_dir=\"/content/\", unigrams_file=\"unigrams.csv\", bigrams_file=\"bigrams.csv\"):\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert to df\n",
        "        df_unigrams = pd.DataFrame([\n",
        "            {\n",
        "                \"word\": word,\n",
        "                \"count\": unigram_counts.get(word, 0),\n",
        "                \"probability\": unigram_probs[word]\n",
        "            }\n",
        "            for word in unigram_probs.keys()\n",
        "        ])\n",
        "\n",
        "        df_bigrams = pd.DataFrame([\n",
        "            {\n",
        "                \"w1\": bigram[0],\n",
        "                \"w2\": bigram[1],\n",
        "                \"bigram\": f\"{bigram[0]} {bigram[1]}\",\n",
        "                \"count\": bigram_counts.get(bigram, 0),\n",
        "                \"probability\": bigram_probs[bigram]\n",
        "            }\n",
        "            for bigram in bigram_probs.keys()\n",
        "        ])\n",
        "\n",
        "        df_unigrams = df_unigrams.sort_values('probability', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        df_bigrams = df_bigrams.sort_values('probability', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        unigram_file = os.path.join(output_dir, unigrams_file)\n",
        "        bigram_file = os.path.join(output_dir, bigrams_file)\n",
        "        df_unigrams.to_csv(unigram_file, index=False)\n",
        "        df_bigrams.to_csv(bigram_file, index=False)\n",
        "\n",
        "        print(\"\\n-------------------------- EXPORT SUMMARY --------------------------\")\n",
        "        print(\"Unigrams exported: \", len(df_unigrams), \"entries\")\n",
        "        print(\"Bigrams exported: \", len(df_bigrams), \"entries\")\n",
        "\n",
        "        return unigram_file, bigram_file, df_unigrams, df_bigrams\n",
        "\n",
        "    def display_top_ngrams(self, df_unigrams, df_bigrams, n=10):\n",
        "        # Display top entries for verification\n",
        "        print(\"\\n-------------------------- TOP \", n, \" UNIGRAMS --------------------------\")\n",
        "        print(df_unigrams.head(n).to_string(index=False))\n",
        "        print(\"\\n-------------------------- TOP \",n,\" BIGRAMS --------------------------\")\n",
        "        print(df_bigrams.head(n).to_string(index=False))\n",
        "\n",
        "    def print_sample_probabilities(self, unigram_probs, bigram_probs, n=10):\n",
        "        print(\"-------------------------- SAMPLE UNIGRAM PROBABILITIES --------------------------\")\n",
        "        count = 0\n",
        "        for token, prob in sorted(unigram_probs.items(), key=lambda x: x[1], reverse=True):\n",
        "          if count >= n:\n",
        "            break\n",
        "          print(f\"P({token}) = {prob:.4f}\")\n",
        "          count += 1\n",
        "\n",
        "        print(\"\\n-------------------------- SAMPLE BIGRAM PROBABILITIES --------------------------\")\n",
        "        count = 0\n",
        "        for (w1, w2), prob in sorted(bigram_probs.items(), key=lambda x: x[1], reverse=True):\n",
        "          if count >= n:\n",
        "            break\n",
        "          print(f\"P({w2}|{w1}) = {prob:.4f}\")\n",
        "          count += 1\n",
        "\n",
        "    def unigram_model_smoothed(self, tokens, k=1.0):\n",
        "        \"\"\"\n",
        "        Build a smoothed unigram model using Add-k (Laplace by default: k=1.0)\n",
        "        Returns smoothed unigram probabilities and counts.\n",
        "        \"\"\"\n",
        "        unigram_counts = {}\n",
        "\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        # Vocabulary size\n",
        "        V = len(unigram_counts)\n",
        "        total_tokens = len(tokens)\n",
        "\n",
        "        unigram_probs = {}\n",
        "        for word in unigram_counts:\n",
        "            unigram_probs[word] = (unigram_counts[word] + k) / (total_tokens + k * V)\n",
        "\n",
        "        # Probability for UNK\n",
        "        # unigram_probs[self.unk_token] = k / (total_tokens + k * V)\n",
        "        return unigram_probs, unigram_counts\n",
        "\n",
        "    def bigram_model_smoothed(self, tokens, k=1.0):\n",
        "        \"\"\"\n",
        "        Build a smoothed bigram model using Add-k smoothing. Returns smoothed bigram probabilities and counts.\n",
        "        \"\"\"\n",
        "        bigram_counts = {}\n",
        "        unigram_counts = {}\n",
        "\n",
        "        for token in tokens:\n",
        "            unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            bigram = (tokens[i], tokens[i+1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "        # Vocabulary size\n",
        "        V = len(unigram_counts)\n",
        "\n",
        "        bigram_probs = {}\n",
        "        for (w1, w2) in bigram_counts:\n",
        "            bigram_probs[(w1, w2)] = (\n",
        "                bigram_counts[(w1, w2)] + k\n",
        "            ) / (unigram_counts[w1] + k * V)\n",
        "\n",
        "        # For all possible bigram pairs (including unseen ones)\n",
        "        all_vocab = list(unigram_counts.keys())\n",
        "        for w1 in all_vocab:\n",
        "            for w2 in all_vocab:\n",
        "                if (w1, w2) not in bigram_probs:\n",
        "                    bigram_probs[(w1, w2)] = k / (unigram_counts[w1] + k * V)\n",
        "\n",
        "        return bigram_probs, bigram_counts\n",
        "\n",
        "    def calculate_perplexity_unigram(self, test_tokens, unigram_probs):\n",
        "        # Function to Calculate perplexity for unigram model\n",
        "        N = len(test_tokens)\n",
        "        log_prob_sum = 0\n",
        "\n",
        "        for token in test_tokens:\n",
        "            if token in unigram_probs:\n",
        "                prob = unigram_probs[token]\n",
        "            else:\n",
        "                # Handle unseen words by assigning probability of them\n",
        "                prob = unigram_probs.get(self.unk_token, 1e-10)\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log(prob)\n",
        "            else:\n",
        "                log_prob_sum += math.log(1e-10)\n",
        "\n",
        "        perplexity = math.exp(-log_prob_sum / N)\n",
        "        return perplexity\n",
        "\n",
        "    def calculate_perplexity_bigram(self, test_tokens, bigram_probs, unigram_probs):\n",
        "        # Function to Calculate perplexity for bigram model\n",
        "        N = len(test_tokens) - 1\n",
        "        if N <= 0:\n",
        "            return float('inf')\n",
        "\n",
        "        log_prob_sum = 0\n",
        "\n",
        "        for i in range(len(test_tokens) - 1):\n",
        "            bigram = (test_tokens[i], test_tokens[i+1])\n",
        "\n",
        "            if bigram in bigram_probs:\n",
        "                prob = bigram_probs[bigram]\n",
        "            else:\n",
        "                # Backoff to unigram\n",
        "                if test_tokens[i+1] in unigram_probs:\n",
        "                    prob = unigram_probs[test_tokens[i+1]]\n",
        "                else:\n",
        "                    prob = unigram_probs.get(self.unk_token, 1e-10)\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log(prob)\n",
        "            else:\n",
        "                log_prob_sum += math.log(1e-10)\n",
        "\n",
        "        perplexity = math.exp(-log_prob_sum / N)\n",
        "        return perplexity"
      ],
      "metadata": {
        "id": "T9hAQBxI3mtT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "RptsRmIbXTPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment = Assignment1()\n",
        "raw_data = assignment.read_data('/content/train.txt')\n",
        "print(\"Read {len(raw_data)} reviews from corpus\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sldtam3REH7",
        "outputId": "b1f27449-4323-4e5c-a45d-8574915e7571"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read {len(raw_data)} reviews from corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = assignment.read_data('/content/val.txt')\n",
        "print(\"Read {len(test_data)} reviews from corpus\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IOVoWjGv2Si",
        "outputId": "b1512681-585c-4afb-fae9-d723ec81f8b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read {len(test_data)} reviews from corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "hZc2yPQdWk-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = assignment.preprocessing(raw_data, remove_stopwords=False)"
      ],
      "metadata": {
        "id": "4SlNx3hpWjLG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_processed_data = assignment.preprocessing(test_data, remove_stopwords=False)"
      ],
      "metadata": {
        "id": "fqErFy8fCSLM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unknown Words handling & Tokenization"
      ],
      "metadata": {
        "id": "C9QleNgIWp-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = assignment.tokenize(processed_data)\n",
        "print(f\"Total tokens after preprocessing: {len(initial_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADU4VQfQWsRi",
        "outputId": "2737a4e2-475e-4204-f99d-4eb17e4a8b27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens after preprocessing: 90761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, initial_unigram_counts = assignment.unigram_model(initial_tokens)\n",
        "print(f\"Initial vocabulary size (unique words): {len(initial_unigram_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ku7CWyXrdKZ",
        "outputId": "ae65735a-a0e6-4af0-f139-2f28d76f0888"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocabulary size (unique words): 6101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace rare words (frequency <= 1) with the <UNK> token.\n",
        "tokens = assignment.handle_unknown_words(initial_tokens, initial_unigram_counts, min_freq=3)\n",
        "print(f\"Total tokens after preprocessing & Unknown Word handling: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaQxW8Hprd__",
        "outputId": "01314d5c-aa46-417b-8608-f3071a093a0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 4345 unique words with frequency <= 3. Replacing them with '<UNK>'.\n",
            "Total tokens after preprocessing & Unknown Word handling: 90761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unknown words handling & Tokenization for Test Data"
      ],
      "metadata": {
        "id": "TUPzMFDc_Lkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_initial_tokens = assignment.tokenize(test_processed_data)\n",
        "print(f\"Total tokens after preprocessing: {len(test_initial_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxVjfoxv_KqC",
        "outputId": "8fcd6799-8e17-4281-f019-02f85acae6be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens after preprocessing: 9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, test_initial_unigram_counts = assignment.unigram_model(test_initial_tokens)\n",
        "print(f\"Initial vocabulary size (unique words): {len(test_initial_unigram_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDe3kOMF_Zj-",
        "outputId": "32efbbf8-52b8-4b1d-9354-7af63210f26a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocabulary size (unique words): 1729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace rare words (frequency <= 1) with the <UNK> token.\n",
        "test_tokens = assignment.handle_unknown_words(test_initial_tokens, test_initial_unigram_counts, min_freq=0)\n",
        "# print(\"Total tokens after preprocessing: {len(test_tokens)}\")"
      ],
      "metadata": {
        "id": "30ggjQE3_htC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute unigram probabilities"
      ],
      "metadata": {
        "id": "Xh6cHtLqXNo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_probabilities, unigram_counts = assignment.unigram_model(tokens)\n",
        "print(f\"Unique unigrams: {len(unigram_probabilities)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-RrvkFwWvVv",
        "outputId": "ff99bfe1-a775-4e08-bd6e-62969bf34e8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique unigrams: 1757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute bigram probabilities"
      ],
      "metadata": {
        "id": "7JARV3W5W97s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_probabilities, bigram_counts = assignment.bigram_model(tokens)\n",
        "print(f\"Unique bigrams: {len(bigram_probabilities)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMMCV5_FWw0s",
        "outputId": "6c568d41-03d5-4d3b-e64e-47a642e20ad0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique bigrams: 27839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample probabilities"
      ],
      "metadata": {
        "id": "VyGmzNZVXC6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment.print_sample_probabilities(unigram_probabilities, bigram_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUX4DUcIWyTg",
        "outputId": "72556eae-6f9b-40d0-d319-718f93f4e75c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------- SAMPLE UNIGRAM PROBABILITIES --------------------------\n",
            "P(<UNK>) = 0.0666\n",
            "P(the) = 0.0583\n",
            "P(.) = 0.0517\n",
            "P(,) = 0.0325\n",
            "P(and) = 0.0286\n",
            "P(a) = 0.0247\n",
            "P(to) = 0.0230\n",
            "P(was) = 0.0201\n",
            "P(i) = 0.0189\n",
            "P(in) = 0.0139\n",
            "\n",
            "-------------------------- SAMPLE BIGRAM PROBABILITIES --------------------------\n",
            "P(suite|junior) = 1.0000\n",
            "P(<UNK>|david) = 1.0000\n",
            "P(room|pump) = 1.0000\n",
            "P(to|due) = 1.0000\n",
            "P(of|sort) = 1.0000\n",
            "P(to|able) = 1.0000\n",
            "P(to|forward) = 1.0000\n",
            "P(club|health) = 1.0000\n",
            "P(n't|ca) = 1.0000\n",
            "P(away|blown) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to CSV files for report"
      ],
      "metadata": {
        "id": "mzE6QXB7XJ3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_file, bigram_file, df_unigrams, df_bigrams = assignment.export_to_csv(\n",
        "    unigram_probabilities, unigram_counts,\n",
        "    bigram_probabilities, bigram_counts\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzlX5KmnWzzs",
        "outputId": "8c12d127-0c54-4c1a-d868-541f1caf7607"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------- EXPORT SUMMARY --------------------------\n",
            "Unigrams exported:  1757 entries\n",
            "Bigrams exported:  27839 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display top n-grams for verification"
      ],
      "metadata": {
        "id": "3Uaz009OXG1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment.display_top_ngrams(df_unigrams, df_bigrams, n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v2GzmLaW1GI",
        "outputId": "547d4ce7-05d6-4aac-b79d-596c53229d9e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------- TOP  10  UNIGRAMS --------------------------\n",
            " word  count  probability\n",
            "<UNK>   6048     0.066637\n",
            "  the   5295     0.058340\n",
            "    .   4692     0.051696\n",
            "    ,   2949     0.032492\n",
            "  and   2593     0.028570\n",
            "    a   2246     0.024746\n",
            "   to   2091     0.023039\n",
            "  was   1828     0.020141\n",
            "    i   1712     0.018863\n",
            "   in   1259     0.013872\n",
            "\n",
            "-------------------------- TOP  10  BIGRAMS --------------------------\n",
            "         w1        w2                bigram  count  probability\n",
            "continental breakfast continental breakfast      5          1.0\n",
            "       pump      room             pump room     12          1.0\n",
            "   supposed        to           supposed to     13          1.0\n",
            "        due        to                due to     16          1.0\n",
            "      based        on              based on     14          1.0\n",
            "       able        to               able to     38          1.0\n",
            "  attention        to          attention to      4          1.0\n",
            "      heart        of              heart of      5          1.0\n",
            "         wo       n't                wo n't     11          1.0\n",
            "       sort        of               sort of      6          1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Laplace Smoothing (k=1)"
      ],
      "metadata": {
        "id": "dYqteLE3qV_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_probs_laplace, unigram_counts = assignment.unigram_model_smoothed(tokens, k=1.0)\n",
        "bigram_probs_laplace, bigram_counts = assignment.bigram_model_smoothed(tokens, k=1.0)"
      ],
      "metadata": {
        "id": "1KzwJitjqGqS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Laplace Smoothed unigram and bigram probabilities"
      ],
      "metadata": {
        "id": "Ax8ajc8-tY0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment.print_sample_probabilities(unigram_probs_laplace, bigram_probs_laplace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cMEJDhHsvRe",
        "outputId": "43352025-64fe-474b-c35d-babebd7ea15d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------- SAMPLE UNIGRAM PROBABILITIES --------------------------\n",
            "P(<UNK>) = 0.0654\n",
            "P(the) = 0.0572\n",
            "P(.) = 0.0507\n",
            "P(,) = 0.0319\n",
            "P(and) = 0.0280\n",
            "P(a) = 0.0243\n",
            "P(to) = 0.0226\n",
            "P(was) = 0.0198\n",
            "P(i) = 0.0185\n",
            "P(in) = 0.0136\n",
            "\n",
            "-------------------------- SAMPLE BIGRAM PROBABILITIES --------------------------\n",
            "P(<s>|</s>) = 0.2257\n",
            "P(the|in) = 0.1349\n",
            "P(the|.) = 0.1347\n",
            "P(the|at) = 0.1331\n",
            "P(the|of) = 0.1226\n",
            "P(the|on) = 0.0955\n",
            "P(<UNK>|a) = 0.0907\n",
            "P(hotel|this) = 0.0889\n",
            "P(.|<UNK>) = 0.0861\n",
            "P(<UNK>|the) = 0.0820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to CSV files of Smoothed Probabilities"
      ],
      "metadata": {
        "id": "l_mY_lJHJEw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_file_laplace, bigram_file_laplace, df_unigrams, df_bigrams = assignment.export_to_csv(\n",
        "    unigram_probs_laplace, unigram_counts,\n",
        "    bigram_probs_laplace, bigram_counts, unigrams_file=\"unigrams_smoothed.csv\", bigrams_file=\"bigrams_smoothed.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y_jOX7b3JD4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3e7b4f-2f7f-4c3a-8564-e8d7fc956f13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------- EXPORT SUMMARY --------------------------\n",
            "Unigrams exported:  1757 entries\n",
            "Bigrams exported:  3087049 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display top smoothed n-grams for verfication"
      ],
      "metadata": {
        "id": "uTagjdUGJQr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment.display_top_ngrams(df_unigrams, df_bigrams, n=10)"
      ],
      "metadata": {
        "id": "s_YoLZEKJP_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee36dc26-3e4c-4872-89d5-a57b14ef747c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------- TOP  10  UNIGRAMS --------------------------\n",
            " word  count  probability\n",
            "<UNK>   6048     0.065382\n",
            "  the   5295     0.057243\n",
            "    .   4692     0.050725\n",
            "    ,   2949     0.031886\n",
            "  and   2593     0.028038\n",
            "    a   2246     0.024287\n",
            "   to   2091     0.022612\n",
            "  was   1828     0.019769\n",
            "    i   1712     0.018515\n",
            "   in   1259     0.013619\n",
            "\n",
            "-------------------------- TOP  10  BIGRAMS --------------------------\n",
            "   w1    w2     bigram  count  probability\n",
            " </s>   <s>   </s> <s>    511     0.225650\n",
            "   in   the     in the    406     0.134947\n",
            "    .   the      . the    868     0.134750\n",
            "   at   the     at the    332     0.133094\n",
            "   of   the     of the    343     0.122594\n",
            "   on   the     on the    228     0.095536\n",
            "    a <UNK>    a <UNK>    362     0.090682\n",
            " this hotel this hotel    208     0.088898\n",
            "<UNK>     .    <UNK> .    671     0.086099\n",
            "  the <UNK>  the <UNK>    577     0.081963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Smoothing: Add-k (k=0.5)"
      ],
      "metadata": {
        "id": "RKawVF0wqff3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_probs_add05, unigram_counts = assignment.unigram_model_smoothed(tokens, k=0.5)\n",
        "bigram_probs_add05, bigram_counts = assignment.bigram_model_smoothed(tokens, k=0.5)"
      ],
      "metadata": {
        "id": "byhLvq4yqk3n"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Add-k Smoothed unigram and bigram probabilities"
      ],
      "metadata": {
        "id": "T-ipjUXWtqmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assignment.print_sample_probabilities(unigram_probs_add05, bigram_probs_add05)"
      ],
      "metadata": {
        "id": "ccAgtCncqnwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b22231-2670-43d4-f4bd-7708c68879b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------- SAMPLE UNIGRAM PROBABILITIES --------------------------\n",
            "P(<UNK>) = 0.0660\n",
            "P(the) = 0.0578\n",
            "P(.) = 0.0512\n",
            "P(,) = 0.0322\n",
            "P(and) = 0.0283\n",
            "P(a) = 0.0245\n",
            "P(to) = 0.0228\n",
            "P(was) = 0.0200\n",
            "P(i) = 0.0187\n",
            "P(in) = 0.0137\n",
            "\n",
            "-------------------------- SAMPLE BIGRAM PROBABILITIES --------------------------\n",
            "P(<s>|</s>) = 0.3679\n",
            "P(the|at) = 0.2048\n",
            "P(the|in) = 0.1902\n",
            "P(the|of) = 0.1782\n",
            "P(the|.) = 0.1559\n",
            "P(the|on) = 0.1505\n",
            "P(hotel|this) = 0.1416\n",
            "P(<num>|$) = 0.1384\n",
            "P(was|it) = 0.1219\n",
            "P(the|from) = 0.1202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the un-smoothed training data"
      ],
      "metadata": {
        "id": "xaN23U7X7RwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = assignment.calculate_perplexity_unigram(tokens, unigram_probabilities)\n",
        "p2 = assignment.calculate_perplexity_bigram(tokens, bigram_probabilities, unigram_probabilities)"
      ],
      "metadata": {
        "id": "rB_P3AgL0Kuu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1"
      ],
      "metadata": {
        "id": "G4_KlztQ0WsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c7fed4-0b6f-42f1-cc14-a2861d1c3efd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247.55057125002799"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2"
      ],
      "metadata": {
        "id": "P_6Bu4Br032H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494c1bdf-55f2-4817-b270-8a0c261f08a7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.29509071773678"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the add-k smoothed training data [with k=0.5]:"
      ],
      "metadata": {
        "id": "n-mHynJx8mcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p3 = assignment.calculate_perplexity_unigram(tokens, unigram_probs_add05)\n",
        "p4 = assignment.calculate_perplexity_bigram(tokens, bigram_probs_add05, unigram_probs_add05)"
      ],
      "metadata": {
        "id": "UNG-fnkj04aO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p3"
      ],
      "metadata": {
        "id": "yRgL7ZWG8-7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d6c9dd-715f-4d7b-8460-fe45175ee26f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247.60531672269684"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p4"
      ],
      "metadata": {
        "id": "Z7o9Da8R8-xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ea1ff1-baef-4549-dd05-fa22135a5343"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138.78916853651518"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the laplace smoothed training data:"
      ],
      "metadata": {
        "id": "JQFIvOsC9FFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p5 = assignment.calculate_perplexity_unigram(tokens, unigram_probs_laplace)\n",
        "p6 = assignment.calculate_perplexity_bigram(tokens, bigram_probs_laplace, unigram_probs_laplace)"
      ],
      "metadata": {
        "id": "dm7T2L5p9M5S"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p5"
      ],
      "metadata": {
        "id": "1MaqnPVx9M0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0deb15-1265-444c-c857-27b5bcb582c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247.75652625415904"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p6"
      ],
      "metadata": {
        "id": "KVu9NItm9MeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b342d8-6086-4edd-c810-0aa2f1082c89"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196.69982420357786"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the un-smoothed test(validation) data"
      ],
      "metadata": {
        "id": "c8m1rPbc-lAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p7 = assignment.calculate_perplexity_unigram(test_tokens, unigram_probabilities)\n",
        "p8 = assignment.calculate_perplexity_bigram(test_tokens, bigram_probabilities, unigram_probabilities)"
      ],
      "metadata": {
        "id": "aM-ItHEq_1v3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p7"
      ],
      "metadata": {
        "id": "yhXfqmmxADKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99be8106-9b04-4899-85cd-5782e2dec79f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231.86077727545114"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p8"
      ],
      "metadata": {
        "id": "fSWvVpEkAEPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2336080-ad91-4a18-a55f-b30d5d77bda3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57.02984483256454"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the laplace smoothed test(validation) data:"
      ],
      "metadata": {
        "id": "pJ0i9KyBAFzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p9 = assignment.calculate_perplexity_unigram(test_tokens, unigram_probs_laplace)\n",
        "p10 = assignment.calculate_perplexity_bigram(test_tokens, bigram_probs_laplace, unigram_probs_laplace)"
      ],
      "metadata": {
        "id": "ZutwdGxfA-XS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p9"
      ],
      "metadata": {
        "id": "FAtPkB14BEin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f083365-ba3d-4031-b146-50e3b865abe8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "232.4233020939776"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p10"
      ],
      "metadata": {
        "id": "YBGaNvwdBEb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be52b56-5660-45e7-9188-adbb9b49d8a9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204.96389763585861"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Perplexity for the add-k smoothed test(validation) data:"
      ],
      "metadata": {
        "id": "0vqjEjk9BGQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p11 = assignment.calculate_perplexity_unigram(test_tokens, unigram_probs_add05)\n",
        "p12 = assignment.calculate_perplexity_bigram(test_tokens, bigram_probs_add05, unigram_probs_add05)"
      ],
      "metadata": {
        "id": "UCQz3tTGBNyp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p11"
      ],
      "metadata": {
        "id": "neZeZoUbBOZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04ff251-83cf-47ef-8c03-6cf4cc10f355"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "232.10437995410112"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p12"
      ],
      "metadata": {
        "id": "Or_DTJnTBOQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c5d8b8-b8c7-4c35-e041-20a0b9571829"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161.55801279313644"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing Add-k Smoothing with k=0.1 and Perplexity [train and validation set]"
      ],
      "metadata": {
        "id": "t57e-yP4YlCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_probs_add01, unigram_counts = assignment.unigram_model_smoothed(tokens, k=0.1)\n",
        "bigram_probs_add01, bigram_counts = assignment.bigram_model_smoothed(tokens, k=0.1)\n",
        "\n",
        "# train set with k=0.1\n",
        "p13 = assignment.calculate_perplexity_unigram(tokens, unigram_probs_add01)\n",
        "p14 = assignment.calculate_perplexity_bigram(tokens, bigram_probs_add01, unigram_probs_add01)\n",
        "\n",
        "# test set with k=0.1\n",
        "p15 = assignment.calculate_perplexity_unigram(test_tokens, unigram_probs_add01)\n",
        "p16 = assignment.calculate_perplexity_bigram(test_tokens, bigram_probs_add01, unigram_probs_add01)"
      ],
      "metadata": {
        "id": "iOVM_HPcW9-5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p13"
      ],
      "metadata": {
        "id": "rgk-Yn-DYekH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b42ff99-d594-4d4b-b0c6-44b92221f2a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247.55287969926633"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p14"
      ],
      "metadata": {
        "id": "EKqafaG5YiLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070960fd-2ee6-499a-d1b3-9d2363f6c5dc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.58610266404538"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p15"
      ],
      "metadata": {
        "id": "gSWeulTCbmhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0efd419-23c0-402b-fb31-3b370147414d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231.9027569696718"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p16"
      ],
      "metadata": {
        "id": "DK5OXegHbn_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cba4cc-384b-45ae-db3c-e94cd7c9dddb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108.37057740343162"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Add-k Smoothing with k=0.05 and Perplexity [train and validation set]"
      ],
      "metadata": {
        "id": "V9A6fWXCex-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_probs_add005, unigram_counts = assignment.unigram_model_smoothed(tokens, k=0.005)\n",
        "bigram_probs_add005, bigram_counts = assignment.bigram_model_smoothed(tokens, k=0.005)\n",
        "\n",
        "# train set with k=0.05\n",
        "p17 = assignment.calculate_perplexity_unigram(tokens, unigram_probs_add005)\n",
        "p18 = assignment.calculate_perplexity_bigram(tokens, bigram_probs_add005, unigram_probs_add005)\n",
        "\n",
        "# test set with k=0.05\n",
        "p19 = assignment.calculate_perplexity_unigram(test_tokens, unigram_probs_add005)\n",
        "p20 = assignment.calculate_perplexity_bigram(test_tokens, bigram_probs_add005, unigram_probs_add005)"
      ],
      "metadata": {
        "id": "BZHHQbTodwP7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p17"
      ],
      "metadata": {
        "id": "6J8KJc38dyVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c87a8c-72c1-4471-e894-d2c69c36cb08"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "247.5505770975103"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p18"
      ],
      "metadata": {
        "id": "VefI14DMeIsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e545dc-a616-4256-86b1-deaa535b44f3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.479045208539695"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p19"
      ],
      "metadata": {
        "id": "XWIfqpb3eUis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b842c1-d7ac-46c8-bdac-cfacd3abfcce"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231.86279082452248"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p20"
      ],
      "metadata": {
        "id": "fMkGl3MleWBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e896de5-a356-45f6-c761-a3df54c8ef11"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.71689407165164"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the Final Results"
      ],
      "metadata": {
        "id": "Y8YoZev9jltx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-------------------------- FINAL RESULTS --------------------------\")\n",
        "print(\"\\n--------------- PERPLEXITY VALUES OF TRAINING DATA --------------\")\n",
        "print(\"\\nFor un-smoothed data: Unigram Model perplexity: \"+ str(p1) + \" and Bigram Model perplexity: \" + str(p2))\n",
        "print(\"\\nFor Laplace smoothed data: Unigram Model perplexity: \"+ str(p5) + \" and Bigram Model perplexity: \" + str(p6))\n",
        "print(\"\\nFor Add-k (k = 0.5) smoothed data: Unigram Model perplexity: \"+ str(p3) + \" and Bigram Model perplexity: \" + str(p4))\n",
        "print(\"\\nFor Add-k (k = 0.1) smoothed data: Unigram Model perplexity: \"+ str(p13) + \" and Bigram Model perplexity: \" + str(p14))\n",
        "print(\"\\nFor Add-k (k = 0.05) smoothed data: Unigram Model perplexity: \"+ str(p17) + \" and Bigram Model perplexity: \" + str(p18))\n",
        "print(\"\\n------------ PERPLEXITY VALUES OF VALIDATION DATA ---------------\")\n",
        "print(\"\\nFor un-smoothed data: Unigram Model validation perplexity is \"+ str(p7) + \" and Bigram Model perplexity: \" + str(p8))\n",
        "print(\"\\nFor Laplace smoothed data: Unigram Model perplexity: \"+ str(p9) + \" and Bigram Model perplexity: \" + str(p10))\n",
        "print(\"\\nFor Add-k (k = 0.5) smoothed data: Unigram Model perplexity: \"+ str(p11) + \" and Bigram Model perplexity: \" + str(p12))\n",
        "print(\"\\nFor Add-k (k = 0.1) smoothed data: Unigram Model perplexity: \"+ str(p15) + \" and Bigram Model perplexity: \" + str(p16))\n",
        "print(\"\\nFor Add-k (k = 0.05) smoothed data: Unigram Model perplexity: \"+ str(p19) + \" and Bigram Model perplexity: \" + str(p20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pkrzcPsjqWX",
        "outputId": "119302e9-eee5-47a8-e57a-3b339c94ff46"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------- FINAL RESULTS --------------------------\n",
            "\n",
            "--------------- PERPLEXITY VALUES OF TRAINING DATA --------------\n",
            "\n",
            "For un-smoothed data: Unigram Model perplexity: 247.55057125002799 and Bigram Model perplexity: 32.29509071773678\n",
            "\n",
            "For Laplace smoothed data: Unigram Model perplexity: 247.75652625415904 and Bigram Model perplexity: 196.69982420357786\n",
            "\n",
            "For Add-k (k = 0.5) smoothed data: Unigram Model perplexity: 247.60531672269684 and Bigram Model perplexity: 138.78916853651518\n",
            "\n",
            "For Add-k (k = 0.1) smoothed data: Unigram Model perplexity: 247.55287969926633 and Bigram Model perplexity: 69.58610266404538\n",
            "\n",
            "For Add-k (k = 0.05) smoothed data: Unigram Model perplexity: 247.5505770975103 and Bigram Model perplexity: 36.479045208539695\n",
            "\n",
            "------------ PERPLEXITY VALUES OF VALIDATION DATA ---------------\n",
            "\n",
            "For un-smoothed data: Unigram Model validation perplexity is 231.86077727545114 and Bigram Model perplexity: 57.02984483256454\n",
            "\n",
            "For Laplace smoothed data: Unigram Model perplexity: 232.4233020939776 and Bigram Model perplexity: 204.96389763585861\n",
            "\n",
            "For Add-k (k = 0.5) smoothed data: Unigram Model perplexity: 232.10437995410112 and Bigram Model perplexity: 161.55801279313644\n",
            "\n",
            "For Add-k (k = 0.1) smoothed data: Unigram Model perplexity: 231.9027569696718 and Bigram Model perplexity: 108.37057740343162\n",
            "\n",
            "For Add-k (k = 0.05) smoothed data: Unigram Model perplexity: 231.86279082452248 and Bigram Model perplexity: 98.71689407165164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\" \" * 25 + \"PERPLEXITY RESULTS\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# Create data for the table\n",
        "data = {\n",
        "    'Smoothing Method': [\n",
        "        'Un-smoothed',\n",
        "        'Laplace (k=1)',\n",
        "        'Add-k (k=0.5)',\n",
        "        'Add-k (k=0.1)',\n",
        "        'Add-k (k=0.05)'\n",
        "    ],\n",
        "    'Training Unigram': [p1, p5, p3, p13, p17],\n",
        "    'Training Bigram': [p2, p6, p4, p14, p18],\n",
        "    'Validation Unigram': [p7, p9, p11, p15, p19],\n",
        "    'Validation Bigram': [p8, p10, p12, p16, p20]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(\"\\n\" + df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "\n",
        "# The Perplexity results are saved to CSV file\n",
        "df.to_csv('perplexity_results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3wquLUeqTlV",
        "outputId": "b1e6cb4b-9f0a-49b7-bf1a-5323dd378395"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "                         PERPLEXITY RESULTS\n",
            "==========================================================================================\n",
            "\n",
            "Smoothing Method  Training Unigram  Training Bigram  Validation Unigram  Validation Bigram\n",
            "     Un-smoothed        247.550571        32.295091          231.860777          57.029845\n",
            "   Laplace (k=1)        247.756526       196.699824          232.423302         204.963898\n",
            "   Add-k (k=0.5)        247.605317       138.789169          232.104380         161.558013\n",
            "   Add-k (k=0.1)        247.552880        69.586103          231.902757         108.370577\n",
            "  Add-k (k=0.05)        247.550577        36.479045          231.862791          98.716894\n",
            "\n",
            "==========================================================================================\n"
          ]
        }
      ]
    }
  ]
}